{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11136873",
   "metadata": {},
   "source": [
    "# The Crawler\n",
    "\n",
    "The crawler consists of 3 different functions: extract_data(), day1_crawler() & compare_records(), and finally our main function which uses them. \n",
    "For our crawler we use selenium - so a chrome driver has been downloaded to May's personal laptop & the entire procees has been running on that laptop.\n",
    "The extract_data() fucntion is first in line - it's used to(as the name suggests) extract all the data we need.\n",
    "In our case we are looking for an item's type, price, condition, amount of pictures uploaded, upload date, if the seller is a private seller or a business, if a description of the item exists and a link to the item.\n",
    "\n",
    "### extract_data() rundown:\n",
    "For each type of data we scrape from Yad2 we use try & except - since we've experienced and learned that sometimes errors accure and the page might just need a refresh and another try.\n",
    "We the use selenium's 'find_element' to track down the relevant html elements we need after inspecting Yad2's different pages.\n",
    "Some elements we track down by class name, some by id and some by xpath.\n",
    "Once an element has been found, it is placed in a dedicated list for that type of element, that later on are used to build a dataframe.\n",
    "\n",
    "### day1_crawler() rundown:\n",
    "This function is only relevant on day number 1 of checking if an item is sold - as on this day we go through our main dataframe from the previous day, and go through the links we've saved, access each link and determine if the item has been sold or not.\n",
    "That is determined by the page that we get when accessing each link - if a page have an element that have the class name 'sorry' this means that the item has been sold.\n",
    "First, we add an empty column for the relevant day of the checkup.\n",
    "We use try & except here as well - for each link we access we try to find the element 'sorry', and if it exists - we add to that row on column 'Is Sold Day #1' the value 1, which indicates item is sold.\n",
    "If 'sorry' does not exist, the 'try' portion of the code will fail and the 'except' will be executed - there we add the value 0 instead meaning the item is still available for purchase.\n",
    "We then return the modified dataframe.\n",
    "\n",
    "### compare_records() rundown:\n",
    "This function is nearly identical to the day1_crawler() function, only that this time we compare the previous day checked to this recent day.\n",
    "If on the previous day an item has been sold, we have no need to go over to the link and check it again, so if on the previous day the item was sold we add the value 1 to today's column.\n",
    "That is the one difference between those two functions.\n",
    "\n",
    "We have three additional functions - isRobot(), isBusiness(), and load_csv().\n",
    "isRobot() is used to help us handle a captcha test.\n",
    "In this function we look for an element only found on a captcha test in Yad2.\n",
    "If the element has been found we return True, else we return false.\n",
    "The isBusiness() fuction helps us figure out if the seller is private or not.\n",
    "Inspecting the item pages we found that an element of 'שם המפרסם' is only found and used when the seller is a business, so if the element is found - return True, else return False.\n",
    "load_csv() simply loads the wanted CSV file and returns it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29595984",
   "metadata": {},
   "source": [
    "### Imports & Chrome Driver PATH set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f664b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "PATH = '/Users/mayvakrat/Downloads/chromedriver'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947835e",
   "metadata": {},
   "source": [
    "### Data Extraction Crawler & Assisting Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a65a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainPage = webdriver.Chrome(PATH)\n",
    "mainPage.get('https://www.yad2.co.il/products/furniture?category=2')\n",
    "itemPage = webdriver.Chrome(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb04b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeDict = []\n",
    "conditionDict = []\n",
    "priceDict = []\n",
    "descriptionDict = []\n",
    "pictureCountDict = []\n",
    "uploadDateDict = []\n",
    "itemLinkDict = []\n",
    "\n",
    "def isRobot(driver):\n",
    "    try:\n",
    "        driver.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def isBusiness():\n",
    "    seller = itemPage.find_elements(by=By.XPATH, value='//*[text()[contains(., \"שם המפרסם\")]]')\n",
    "    if len(seller) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def extractData():\n",
    "    \n",
    "    try:\n",
    "        fur_type = itemPage.find_element(by=By.CLASS_NAME, value='sub_category_title').text.split('-')[1].strip()\n",
    "        typeDict.append(fur_type)\n",
    "    except:\n",
    "        itemPage.refresh()\n",
    "        time.sleep(1)\n",
    "        fur_type = itemPage.find_element(by=By.CLASS_NAME, value='sub_category_title').text.split('-')[1].strip()\n",
    "        typeDict.append(fur_type)\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            pic_count = itemPage.find_element(by=By.CLASS_NAME, value='swiper-pagination-total').text\n",
    "            pictureCountDict.append(pic_count)\n",
    "        except:\n",
    "            pictureCountDict.append(0)\n",
    "    except:\n",
    "        itemPage.refresh()\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            pic_count = itemPage.find_element(by=By.CLASS_NAME, value='swiper-pagination-total').text\n",
    "            pictureCountDict.append(pic_count)\n",
    "        except:\n",
    "            pictureCountDict.append(0)\n",
    "\n",
    "    try:\n",
    "        price = itemPage.find_element(by=By.CLASS_NAME, value='classified_price').text\n",
    "        priceDict.append(price)\n",
    "    except:\n",
    "        itemPage.refresh()\n",
    "        time.sleep(1)\n",
    "        price = itemPage.find_element(by=By.CLASS_NAME, value='classified_price').text\n",
    "        priceDict.append(price)\n",
    "\n",
    "    try:\n",
    "        condition = itemPage.find_elements(by=By.XPATH, value='//*[text()[contains(., \"מצב המוצר\")]]')\n",
    "        for item in condition:\n",
    "            if item.tag_name == 'dt':\n",
    "                currentElement = item\n",
    "        sibling = currentElement.find_element(by=By.XPATH, value='following-sibling::*').text\n",
    "        conditionDict.append(sibling)\n",
    "    except:\n",
    "        itemPage.refresh()\n",
    "        time.sleep(1)\n",
    "        condition = itemPage.find_elements(by=By.XPATH, value='//*[text()[contains(., \"מצב המוצר\")]]')\n",
    "        for item in condition:\n",
    "            if item.tag_name == 'dt':\n",
    "                currentElement = item\n",
    "        sibling = currentElement.find_element(by=By.XPATH, value='following-sibling::*').text\n",
    "        conditionDict.append(sibling)\n",
    "\n",
    "    try:\n",
    "        upload = itemPage.find_elements(by=By.XPATH, value='//*[text()[contains(., \"תאריך עדכון\")]]')\n",
    "        for item in upload:\n",
    "            if item.tag_name == 'dt':\n",
    "                currentElement = item\n",
    "        sibling = currentElement.find_element(by=By.XPATH, value='following-sibling::*').text\n",
    "        if sibling == 'עודכן היום':\n",
    "            uploadDateDict.append(datetime.date(datetime.now()))\n",
    "        else:\n",
    "            uploadDateDict.append(sibling)\n",
    "    except:\n",
    "        itemPage.refresh()\n",
    "        time.sleep(1)\n",
    "        upload = itemPage.find_elements(by=By.XPATH, value='//*[text()[contains(., \"תאריך עדכון\")]]')\n",
    "        for item in upload:\n",
    "            if item.tag_name == 'dt':\n",
    "                currentElement = item\n",
    "        sibling = currentElement.find_element(by=By.XPATH, value='following-sibling::*').text\n",
    "        if sibling == 'עודכן היום':\n",
    "            uploadDateDict.append(datetime.date(datetime.now()))\n",
    "        else:\n",
    "            uploadDateDict.append(sibling)\n",
    "\n",
    "    try:\n",
    "        description = itemPage.find_element(by=By.CLASS_NAME, value='details_text').text\n",
    "        if description == None:\n",
    "            descriptionDict.append('no')\n",
    "        else:\n",
    "            descriptionDict.append('yes')\n",
    "    except:\n",
    "        itemPage.refresh()\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            description = itemPage.find_element(by=By.CLASS_NAME, value='details_text').text\n",
    "            if description == None:\n",
    "                descriptionDict.append('no')\n",
    "            else:\n",
    "                descriptionDict.append('yes')\n",
    "        except:\n",
    "            descriptionDict.append('no')\n",
    "    \n",
    "    try:\n",
    "        itemLink = itemPage.current_url\n",
    "        itemLinkDict.append(itemLink)\n",
    "    except:\n",
    "        itemPage.refresh()\n",
    "        time.sleep(1)\n",
    "        itemLink = itemPage.current_url\n",
    "        itemLinkDict.append(itemLink)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009646b",
   "metadata": {},
   "source": [
    "### Remaining Crawlers & Assisting Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f975672",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemPage = webdriver.Chrome(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521bb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv):\n",
    "    df = pd.read_csv(csv, index_col=[0])\n",
    "    return df\n",
    "\n",
    "def day1_crawler(main_df):\n",
    "    df_copy = main_df.copy()\n",
    "    df_copy['Is Sold Day #1'] = np.nan\n",
    "    for index, row in df_copy.iterrows():\n",
    "        try:\n",
    "            link = row['Link']\n",
    "            itemPage.get(link)\n",
    "\n",
    "            if isRobot(itemPage) == True:\n",
    "                try:\n",
    "                    element = itemPage.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]')\n",
    "                    time.sleep(5)\n",
    "                    while element == itemPage.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]'):\n",
    "                        WebmaDriverWait(mainPage, 99999).until(EC.staleness_of(element))\n",
    "                        time.sleep(5)\n",
    "                except:\n",
    "                    time.sleep(5)\n",
    "\n",
    "            sorry = itemPage.find_element_by_class_name('sorry')\n",
    "            df_copy.at[index, 'Is Sold Day #1'] = 1\n",
    "        except:   \n",
    "            df_copy.at[index, 'Is Sold Day #1'] = 0   \n",
    "    df_copy = df_copy.astype({'Is Sold Day #1':'int'})\n",
    "    return df_copy\n",
    "    \n",
    "\n",
    "def compare_records(main_df, prev_day_col, curr_day_col):\n",
    "    df_copy = main_df.copy()\n",
    "    new_prev_col = 'Is Sold Day #' + str(prev_day_col)\n",
    "    new_curr_col = 'Is Sold Day #' + str(curr_day_col)\n",
    "    df_copy[new_curr_col] = np.nan\n",
    "    for index, row in df_copy.iterrows():\n",
    "        if df_copy.at[index, new_prev_col] == 1:\n",
    "            df_copy.at[index, new_curr_col] = 1\n",
    "        else:\n",
    "            try:\n",
    "                link = row['Link']\n",
    "                itemPage.get(link)\n",
    "                \n",
    "                if isRobot(itemPage) == True:\n",
    "                    try:\n",
    "                        element = itemPage.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]')\n",
    "                        time.sleep(5)\n",
    "                        while element == itemPage.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]'):\n",
    "                            WebmaDriverWait(mainPage, 99999).until(EC.staleness_of(element))\n",
    "                            time.sleep(5)\n",
    "                    except:\n",
    "                        time.sleep(5)\n",
    "                \n",
    "                sorry = itemPage.find_element_by_class_name('sorry')\n",
    "                df_copy.at[index, new_curr_col] = 1\n",
    "            except:  \n",
    "                df_copy.at[index, new_curr_col] = 0  \n",
    "    df_copy = df_copy.astype({new_curr_col:'int'})\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72818f",
   "metadata": {},
   "source": [
    "### Data Extraction Main Code & Main Dataframe Creation\n",
    "\n",
    "For data extraction we use a while loop that run 800 times - for 800 different item.\n",
    "We also address page changes when we are at the last item of the page.\n",
    "In addition we ignore first three items as the are always posted by a business seller - something we are not interested in, and are built different from other items.\n",
    "\n",
    "We use two driver instances - MainPage which is the second hand furniture page on Yad2, and itemPage which is the page we access links of each item.\n",
    "On mainPage we find the item elements, extract their item-id which we use to make a link, and we go through pages.\n",
    "On itemPage we access that link we made on mainPage, and extract all the information we need with the extract_data() function.\n",
    "\n",
    "We use the isRobot() function both on mainPage and itemPage on each itteration to determine if we need to handle a captcha test or not.\n",
    "If the function return the value True, we then go on a while loop - as long as the element only relevant to the captcha test exists - we do not go forward(there was a need for a loop since sometimes, the test repeats itself multiple times).\n",
    "Once we handle that, we go on to check if the seller is a business seller - if so, we break the current itteration of the loop and continue on to the next one, as we are uninterested in business sellers.\n",
    "\n",
    "When we are done extracting the data into designated lists, we make those lists into a dataframe and save it locally, and terminate the crawling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemList = mainPage.find_element(by=By.CSS_SELECTOR, value=\"div[class='feed_list gallery_item new_gallery_view_design']\")\n",
    "itemCount = len(itemList.find_elements(by=By.CSS_SELECTOR, value=\"div[class='feeditem table']\")) + 3\n",
    "i = 0\n",
    "itemIndex = 3\n",
    "\n",
    "while i < 800:\n",
    "        \n",
    "    if itemIndex == itemCount:\n",
    "        itemIndex = 3\n",
    "        try:\n",
    "            nextPage = mainPage.find_element(by=By.CSS_SELECTOR, value=\"span[class='navigation-button-text next-text']\")\n",
    "            nextPage.click()\n",
    "            itemList = mainPage.find_element(by=By.CSS_SELECTOR, value=\"div[class='feed_list gallery_item new_gallery_view_design']\")\n",
    "            itemCount = len(itemList.find_elements(by=By.CSS_SELECTOR, value=\"div[class='feeditem table']\")) + 3\n",
    "        except:\n",
    "            mainPage.quit()\n",
    "\n",
    "    try:\n",
    "        item = mainPage.find_element_by_id('feed_item_'+str(itemIndex))\n",
    "        itemID = item.get_attribute(\"item-id\")\n",
    "        itemPage.get('https://www.yad2.co.il/item/'+str(itemID))\n",
    "    except:\n",
    "        mainPage.refresh()\n",
    "        itemIndex = itemIndex + 1\n",
    "        item = mainPage.find_element_by_id('feed_item_'+str(itemIndex))\n",
    "        itemID = item.get_attribute(\"item-id\")\n",
    "        itemPage.get('https://www.yad2.co.il/item/'+itemID)\n",
    "    \n",
    "    if isRobot(mainPage) == True:\n",
    "        try:\n",
    "            element = mainPage.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]')\n",
    "            time.sleep(5)\n",
    "            while element == mainPage.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]'):\n",
    "                WebmaDriverWait(mainPage, 99999).until(EC.staleness_of(element))\n",
    "                time.sleep(5)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "        \n",
    "    if isRobot(itemPage) == True:\n",
    "        try:\n",
    "            element = itemPage.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]')\n",
    "            time.sleep(5)\n",
    "            while element == itemPage.find_element(by=By.XPATH, value='//*[text()[contains(., \"?Are you for real\")]]'):\n",
    "                WebDriverWait(itemPage, 99999).until(EC.staleness_of(element))\n",
    "                time.sleep(5)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    \n",
    "    if isBusiness() == True:\n",
    "        itemIndex += 1\n",
    "        continue\n",
    "    \n",
    "    extractData()\n",
    "    \n",
    "    itemIndex += 1\n",
    "    i += 1\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "mainPage.quit()\n",
    "itemPage.quit()\n",
    "\n",
    "columns = ['Link', 'Type', 'Price', 'Condition', 'Is description', 'Picture count', 'Upload Date', 'Is Sold Day #1','Is Sold Day #2','Is Sold Day #3', 'Is Sold Day #4','Is Sold Day #5', 'Catagory mean', 'Days until sold', 'Price to Catagory mean', 'Is change in price']\n",
    "df = pd.DataFrame({'Link': itemLinkDict, 'Type': typeDict, 'Price': priceDict, 'Condition': conditionDict,'Is description': descriptionDict, 'Picture count': pictureCountDict, 'Upload Date': uploadDateDict})\n",
    "\n",
    "df.to_csv(\"/Users/mayvakrat/Desktop/School Shit/Year 2/Semester B/Introduction To Data Science/Final Project/DF's/df-Day1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182c1a3",
   "metadata": {},
   "source": [
    "### Day #1 Crawler Execution:\n",
    "\n",
    "We load our main dataframe - the one containing all the data we scraped from Yad2.\n",
    "An important piece of data we collected was the link for each add we extracted information from, so we can access each ad we saved to check if it has been sold or not.\n",
    "for that we first use 'day1_crawler' to go through the links of each item and check existence - since we have no previous day to compare existance to.\n",
    "We then terminate the crawling process, and save our updated dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemPage = webdriver.Chrome(PATH)\n",
    "\n",
    "main_df = load_csv(\"DF's/df-initial data.csv\")\n",
    "new_df = day1_crawler(main_df)\n",
    "itemPage.quit()\n",
    "new_df.to_csv(\"/Users/mayvakrat/Desktop/School Shit/Year 2/Semester B/Introduction To Data Science/Final Project/DF's/df-day1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3287d59",
   "metadata": {},
   "source": [
    "### Remaining Days Crawling:\n",
    "Since we now have a column from day #1's checkup, we can use our 'compare_records' function for the remaining four days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "436f8685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/bjmsrq3s0t9_vrv_xp_hbg0c0000gn/T/xpython_12977/639622082.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  itemPage = webdriver.Chrome(PATH)\n",
      "/var/folders/15/bjmsrq3s0t9_vrv_xp_hbg0c0000gn/T/xpython_12977/3604320285.py:54: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  sorry = itemPage.find_element_by_class_name('sorry')\n"
     ]
    }
   ],
   "source": [
    "itemPage = webdriver.Chrome(PATH)\n",
    "main_df = load_csv(\"DF's/df-day4.csv\")\n",
    "prev_day = 4\n",
    "curr_day = 5\n",
    "new_df = compare_records(main_df, prev_day, curr_day)\n",
    "itemPage.quit()\n",
    "new_df.to_csv(\"/Users/mayvakrat/Desktop/School Shit/Year 2/Semester B/Introduction To Data Science/Final Project/DF's/df-day5.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
